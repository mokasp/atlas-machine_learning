{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhUUMQ2WLTu9",
        "outputId": "3fa1d2ed-d7c1-4dcc-8930-0ac0ad4a1287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.7\n",
            "  Downloading gym-0.7.0.tar.gz (149 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/149.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.7) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.7) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gym==0.7) (1.16.0)\n",
            "Collecting pyglet>=1.2.0 (from gym==0.7)\n",
            "  Downloading pyglet-2.0.17-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7) (2024.8.30)\n",
            "Downloading pyglet-2.0.17-py3-none-any.whl (936 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.6/936.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.7.0-py3-none-any.whl size=201576 sha256=371e8525d90ea3e1b1735210af4d1aeff94b8e66073c9e495de60c80e614a370\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/41/63/c8ad08982323c2d78191ea49280294935111fd8e5010534f85\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.7.0 pyglet-2.0.17\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.7\n",
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Monte Carlo\n",
        "Write the function `def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):` that performs the Monte Carlo algorithm:\n",
        "\n",
        "\n",
        "- `env` is the openAI environment instance\n",
        "- `V` is a `numpy.ndarray` of shape `(s,)` containing the value estimate\n",
        "- `policy` is a function that takes in a state and returns the next action to take\n",
        "- `episodes` is the total number of episodes to train over\n",
        "- `max_steps` is the maximum number of steps per episode\n",
        "- `alpha` is the learning rate\n",
        "- `gamma` is the discount rate\n",
        "- Returns: `V`, the updated value estimate"
      ],
      "metadata": {
        "id": "K50DduJ2LnjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=4)\n",
        "env.seed(0)\n",
        "print(monte_carlo(env, V, policy).reshape((8, 8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YxCiSEj4hrq",
        "outputId": "a6ca1dc5-0791-423a-f5e7-8825ac504b5b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake8x8-v0\n",
            "[2024-09-20 11:47:51,023] Making new env: FrozenLake8x8-v0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.81    0.9     0.4783  0.4305  0.3874  0.4305  0.6561  0.9   ]\n",
            " [ 0.9     0.729   0.5905  0.4783  0.5905  0.2824  0.2824  0.3874]\n",
            " [ 1.      0.5314  0.729  -1.      1.      0.3874  0.2824  0.4305]\n",
            " [ 1.      0.5905  0.81    0.9     1.     -1.      0.3874  0.6561]\n",
            " [ 1.      0.6561  0.81   -1.      1.      1.      0.729   0.5314]\n",
            " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
            " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
            " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo(env, V, policy, episodes=5000,\n",
        "                max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    \"\"\" function that performs the monte carlo algorithm\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): gym environment instance.\n",
        "            V (numpy.ndarray): value estimate\n",
        "            policy (function): takes in a state and returns the next action\n",
        "                to take\n",
        "            episodes (int): total number of episodes to train over,\n",
        "                default is 5000.\n",
        "            max_steps (int): maximum number of steps per episode,\n",
        "                default is 100.\n",
        "            alpha (float): learning rate for value estimate update,\n",
        "                default is 0.1.\n",
        "            gamma (float): discount rate for future rewards, default is 0.99.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: V\n",
        "                - V (numpy.ndarray): updated value estimate\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(episodes):\n",
        "        # reset environment\n",
        "        state = env.reset()\n",
        "\n",
        "        # keep track of each state and its reward\n",
        "        states = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            # get new action\n",
        "            action = policy(state)\n",
        "\n",
        "            # take a step and document the states and rewards\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # go to next state\n",
        "            state = next_state\n",
        "\n",
        "            # check if episode is finished\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # keep track of the accumulated reward\n",
        "        returns = 0\n",
        "        for step in range(len(states) - 1, -1, -1):\n",
        "\n",
        "            # get the state and reward from this timestep for this episode\n",
        "            state = states[step]\n",
        "            reward = rewards[step]\n",
        "\n",
        "            # calculate accumulated reward\n",
        "            returns = (gamma * returns) + reward\n",
        "\n",
        "            # if the state hasnt been visited yet in this episode\n",
        "            if state not in states[:i]:\n",
        "\n",
        "                # update the value estimate\n",
        "                V[state] += alpha * (returns - V[state])\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "8EZZiHWrsFAZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TD(λ)\n",
        "Write the function `def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):` that performs the TD(λ) algorithm:\n",
        "\n",
        "- `env` is the openAI environment instance\n",
        "- `V` is a `numpy.ndarray` of shape `(s,)` containing the value estimate\n",
        "- `policy` is a function that takes in a state and returns the next action to take\n",
        "- `lambtha` is the eligibility trace factor\n",
        "- `episodes` is the total number of episodes to train over\n",
        "- `max_steps` is the maximum number of steps per episode\n",
        "- `alpha` is the learning rate\n",
        "- `gamma` is the discount rate\n",
        "- Returns: `V`, the updated value estimate"
      ],
      "metadata": {
        "id": "XN8GC6roMWbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=4)\n",
        "env.seed(0)\n",
        "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsTZ1K3Augoo",
        "outputId": "c1b26416-4165-4eb0-c566-1256a1fd96f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake8x8-v0\n",
            "[2024-09-20 11:47:31,975] Making new env: FrozenLake8x8-v0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.774  -0.8288 -0.8065 -0.7214 -0.6344 -0.548  -0.4152 -0.4393]\n",
            " [-0.7643 -0.7553 -0.776  -0.6273 -0.4213 -0.4698 -0.3294 -0.4009]\n",
            " [-0.8883 -0.8796 -0.9215 -1.     -0.669  -0.37   -0.2522 -0.4788]\n",
            " [-0.9091 -0.907  -0.9199 -0.9078 -0.8009 -1.     -0.3478 -0.1532]\n",
            " [-0.8774 -0.9579 -0.9336 -1.     -0.7624 -0.8244 -0.6629 -0.1192]\n",
            " [-0.9308 -1.     -1.      0.6361 -0.7978 -0.715  -1.      0.3673]\n",
            " [-0.9145 -1.     -0.5743 -0.0703 -1.     -0.3774 -1.      0.9231]\n",
            " [-0.8599 -0.8444 -0.7795 -1.      1.      0.4657  0.5018  1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def td_lambtha(env, V, policy, lambtha, episodes=5000,\n",
        "               max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    \"\"\" function that performs the TD(λ) algorithm\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): gym environment instance.\n",
        "            V (numpy.ndarray): value estimate\n",
        "            policy (function): takes in a state and returns the next action\n",
        "                to take\n",
        "            lambtha (float):  eligibility trace factor\n",
        "            episodes (int): total number of episodes to train over,\n",
        "                default is 5000.\n",
        "            max_steps (int): maximum number of steps per episode,\n",
        "                default is 100.\n",
        "            alpha (float): learning rate, default is 0.1.\n",
        "            gamma (float): discount rate for future rewards, default is 0.99.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: V\n",
        "                - V (numpy.ndarray): updated value estimate\n",
        "    \"\"\"\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        # reset the environment and EoE flag\n",
        "        state = env.reset()\n",
        "        end_of_episode = False\n",
        "\n",
        "        # initialize empty array to keep track of which states have been\n",
        "        # visited and how many times\n",
        "        eligibilty_trace = np.zeros_like(V)\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            # use policy to decide which action to take\n",
        "            action = policy(state)\n",
        "\n",
        "            # use selected action and get new reward\n",
        "            current_state, reward, end_of_episode, _ = env.step(action)\n",
        "\n",
        "            # caculate the temporal difference error\n",
        "            td_error = reward + gamma * V[current_state] - V[state]\n",
        "            # add one each time a certain state is visited\n",
        "            eligibilty_trace[state] += 1\n",
        "\n",
        "            # update the value estimate using the eligibilty trace\n",
        "            V += alpha * td_error * eligibilty_trace\n",
        "\n",
        "            # decay the eligibility trace\n",
        "            eligibilty_trace *= gamma * lambtha\n",
        "\n",
        "            # go to next state\n",
        "            state = current_state\n",
        "\n",
        "            # check if episode is over\n",
        "            if end_of_episode:\n",
        "                break\n",
        "\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "sFOL13ruND_N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. SARSA(λ)\n",
        "\n",
        "Write the function `def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):` that performs SARSA(λ):\n",
        "\n",
        "- `env` is the openAI environment instance\n",
        "- `Q` is a `numpy.ndarray` of shape `(s,a)` containing the Q table\n",
        "- `lambtha` is the eligibility trace factor\n",
        "- `episodes` is the total number of episodes to train over\n",
        "- `max_steps` is the maximum number of steps per episode\n",
        "- `alpha` is the learning rate\n",
        "- `gamma` is the discount rate\n",
        "- `epsilon` is the initial threshold for epsilon greedy\n",
        "- `min_epsilon` is the minimum value that epsilon should decay to\n",
        "- `epsilon_decay` is the decay rate for updating epsilon between episodes\n",
        "- Returns: `Q`, the updated Q table"
      ],
      "metadata": {
        "id": "n7qq_R9hNOih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ep_greedy_policy(Q, state, epsilon):\n",
        "    \"\"\" function that performs the epsilon greedy policy\n",
        "\n",
        "        Args:\n",
        "            Q (numpy.ndarray): Q-table where rows represent states and columns\n",
        "                represent actions.\n",
        "            state (int): the current state of the agent\n",
        "            epsilon (float): initial exploration rate for the epsilon-greedy\n",
        "                policy, default is 1.\n",
        "    \"\"\"\n",
        "    # use epsilon-greedy to decide the next move\n",
        "    # choose a random value between 0 and 1\n",
        "    choice = np.random.random()\n",
        "    # decide to explore or exploit based on the epsilon\n",
        "    if choice < epsilon:\n",
        "        # exploration: pick a random action\n",
        "        action = np.random.randint(len(Q[state]))\n",
        "    else:\n",
        "        # exploitation: pick the action with highest q-value\n",
        "        action = np.argmax(Q[state, :])\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
        "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
        "    \"\"\" function that that performs SARSA(λ)\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): gym environment instance.\n",
        "            Q (numpy.ndarray): Q-table where rows represent states and columns\n",
        "                represent actions.\n",
        "            lambtha (float):  eligibility trace factor\n",
        "            episodes (int): total number of episodes to train over,\n",
        "                default is 5000.\n",
        "            max_steps (int): maximum number of steps per episode,\n",
        "                default is 100.\n",
        "            alpha (float): learning rate for Q-learning, default is 0.1.\n",
        "            gamma (float): discount rate for future rewards, default is 0.99.\n",
        "            epsilon (float): initial exploration rate for the epsilon-greedy\n",
        "                policy, default is 1.\n",
        "            min_epsilon (float): minimum value that epsilon should decay to,\n",
        "                default is 0.1.\n",
        "            epsilon_decay (float): rate at which epsilon decays after each\n",
        "                episode, default is 0.05.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Q\n",
        "                - Q (numpy.ndarray): updated Q-table after sarsa lambda.\n",
        "    \"\"\"\n",
        "    for episode in range(episodes):\n",
        "        # reset the environment and EoE flag\n",
        "        state = env.reset()\n",
        "        end_of_episode = False\n",
        "        eligibility_trace = np.zeros_like(Q)\n",
        "        action = ep_greedy_policy(Q, state, epsilon)\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "\n",
        "            # use selected action and get new reward\n",
        "            current_state, reward, end_of_episode, _ = env.step(action)\n",
        "\n",
        "            # find next action for the td error\n",
        "            next_action = ep_greedy_policy(Q, current_state, epsilon)\n",
        "\n",
        "            # calculate the temporal difference error using the immediate\n",
        "            # reward and the discounted estimate of the future rewards\n",
        "            td_error = reward + \\\n",
        "                (gamma * Q[current_state, next_action]) - Q[state, action]\n",
        "\n",
        "            # add one each time a certain state is visited\n",
        "            eligibility_trace[state, action] += 1\n",
        "\n",
        "            # update the Q value using the eligibility trace\n",
        "            Q[state, action] = Q[state, action] + alpha * \\\n",
        "                td_error * eligibility_trace[state, action]\n",
        "\n",
        "            # decay the eligibility trace\n",
        "            eligibility_trace *= gamma * lambtha\n",
        "\n",
        "            # set the new state and action\n",
        "            state = current_state\n",
        "            action = next_action\n",
        "\n",
        "            # check if the episode is complete\n",
        "            if end_of_episode:\n",
        "                break\n",
        "\n",
        "        # epsilon decay - as the agent explores throughout training, we can\n",
        "        # reduce the epsilon to encourage exploitation\n",
        "        epsilon = min_epsilon + (epsilon - min_epsilon) * \\\n",
        "            np.exp(-epsilon_decay * episode)\n",
        "\n",
        "    return Q\n"
      ],
      "metadata": {
        "id": "rD1goWEFNrCb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "Q = np.random.uniform(size=(64, 4))\n",
        "np.set_printoptions(precision=4)\n",
        "env.seed(0)\n",
        "print(sarsa_lambtha(env, Q, 0.9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIQLOxas1cyd",
        "outputId": "127280d6-2a2e-4f5a-a4a2-35ccba5b1634"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake8x8-v0\n",
            "[2024-09-20 11:47:19,882] Making new env: FrozenLake8x8-v0\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:18: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6189 0.6679 0.6419 0.6232]\n",
            " [0.6605 0.6211 0.6169 0.6075]\n",
            " [0.638  0.5846 0.5795 0.5705]\n",
            " [0.4008 0.5194 0.317  0.3719]\n",
            " [0.3511 0.4706 0.4719 0.4602]\n",
            " [0.463  0.461  0.4824 0.4369]\n",
            " [0.3563 0.4596 0.3743 0.481 ]\n",
            " [0.4513 0.4234 0.4002 0.395 ]\n",
            " [0.6523 0.6906 0.6434 0.6497]\n",
            " [0.6921 0.6352 0.6378 0.6332]\n",
            " [0.6354 0.5387 0.5762 0.5833]\n",
            " [0.326  0.4566 0.3632 0.3228]\n",
            " [0.4371 0.4556 0.49   0.411 ]\n",
            " [0.566  0.3604 0.3804 0.2725]\n",
            " [0.5469 0.2942 0.4672 0.2923]\n",
            " [0.159  0.2049 0.5482 0.1812]\n",
            " [0.7034 0.7204 0.6864 0.6745]\n",
            " [0.6763 0.7257 0.6648 0.6513]\n",
            " [0.6942 0.5519 0.5537 0.4395]\n",
            " [0.2828 0.1202 0.2961 0.1187]\n",
            " [0.4236 0.496  0.3147 0.4507]\n",
            " [0.5585 0.4804 0.7179 0.2561]\n",
            " [0.5519 0.5437 0.4268 0.5985]\n",
            " [0.244  0.6455 0.3958 0.3561]\n",
            " [0.7232 0.7526 0.7191 0.7237]\n",
            " [0.7093 0.7441 0.7324 0.7189]\n",
            " [0.6982 0.7739 0.747  0.729 ]\n",
            " [0.73   0.7849 0.5068 0.646 ]\n",
            " [0.6839 0.7876 0.6701 0.6463]\n",
            " [0.8811 0.5813 0.8817 0.6925]\n",
            " [0.7589 0.5385 0.6984 0.6519]\n",
            " [0.4696 0.6705 0.1542 0.3001]\n",
            " [0.775  0.7348 0.7504 0.7458]\n",
            " [0.7963 0.7138 0.7466 0.7479]\n",
            " [0.7289 0.7899 0.7292 0.7426]\n",
            " [0.8965 0.3676 0.4359 0.8919]\n",
            " [0.7526 0.846  0.4464 0.7293]\n",
            " [0.7142 0.7819 0.2051 0.7213]\n",
            " [0.2856 0.6233 0.1238 0.6961]\n",
            " [0.7876 0.5913 0.489  0.1322]\n",
            " [0.6047 0.6334 0.5957 0.8723]\n",
            " [0.9755 0.8558 0.0117 0.36  ]\n",
            " [0.73   0.1716 0.521  0.0543]\n",
            " [0.2    0.0853 0.844  0.2749]\n",
            " [0.4089 0.7456 0.82   0.2847]\n",
            " [0.2264 0.8325 0.612  0.2905]\n",
            " [0.9342 0.614  0.5356 0.5899]\n",
            " [0.969  0.3119 0.3982 0.3373]\n",
            " [0.3806 0.4534 0.5222 0.4878]\n",
            " [0.2274 0.2544 0.058  0.4344]\n",
            " [0.3118 0.6331 0.3778 0.1796]\n",
            " [0.0247 0.0672 0.7475 0.4537]\n",
            " [0.5366 0.8967 0.9903 0.2169]\n",
            " [0.6948 0.308  0.0207 0.8436]\n",
            " [0.32   0.3835 0.5883 0.831 ]\n",
            " [0.629  1.1819 0.2735 0.798 ]\n",
            " [0.3482 0.4065 0.4657 0.3586]\n",
            " [0.4361 0.4057 0.2813 0.2433]\n",
            " [0.5827 0.1204 0.2075 0.4471]\n",
            " [0.3742 0.4636 0.2776 0.5868]\n",
            " [0.8234 0.1175 0.5174 0.1321]\n",
            " [0.7307 0.3961 0.5654 0.1833]\n",
            " [0.1448 0.4881 0.3556 0.9404]\n",
            " [0.7653 0.7487 0.9037 0.0834]]\n"
          ]
        }
      ]
    }
  ]
}